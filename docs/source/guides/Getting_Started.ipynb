{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b14b8e7c-79f8-491c-ad6e-7a57c1103425",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "In this notebook we use boax to demonstrate a single step of a typical bayesion optimization process.\n",
    "\n",
    "We will begin by defining the latent objective function we want to maximize, as well as the bounded search space that guides the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa1a037-0e28-4023-8efc-046febb4f664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import config\n",
    "\n",
    "# Double precision is highly recommended.\n",
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c736af-ef31-4b7c-bfc1-0483e70ca346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from jax import jit\n",
    "from jax import lax\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "from jax import scipy\n",
    "from jax import value_and_grad\n",
    "from jax import vmap\n",
    "\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from boax.prediction import bijectors, kernels, means, processes\n",
    "from boax.optimization import acquisitions, maximizers, spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3c4882-c610-4448-8699-545cecb97631",
   "metadata": {},
   "source": [
    "As our latent objective function we chose a sinusoid that we aim to maximize in the interval of [-3, 3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba0aa909-2832-47e2-9427-01cd163b712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = spaces.continuous(jnp.array([[-3, 3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5385417b-ebb4-4e75-89aa-0215d21e968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x):\n",
    "  return jnp.sin(4 * x[..., 0]) + jnp.cos(2 * x[..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00407e4b-1c72-4540-a08a-accd1216d277",
   "metadata": {},
   "source": [
    "To create the observation training data we sample random points from a uniform distribution, evaluate the objective functions at those points, and finish by adding gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce6f727-86ed-4f31-aeda-ea380ee228fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_key, noise_key = random.split(random.key(0))\n",
    "x_train = random.uniform(sample_key, minval=-3, maxval=3, shape=(10, 1))\n",
    "y_train = objective(x_train) + 0.3 * random.normal(noise_key, shape=(10,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd846011-f012-4d34-8529-c74ede9a6adf",
   "metadata": {},
   "source": [
    "## Fitting a Gaussian Process model to the data\n",
    "\n",
    "With the observations in place, we can now focus on constructing a Gaussian Process model and fit it to the data. For this example we choose a simple setup of a constant zero mean function and a scaled RBF kernel. Note that we use the softplus bijector to constrain some of the models' hyperparameters to be strictly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a3bb423-202e-43d6-bb9d-45f8d37825d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bijector = bijectors.softplus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "800c903f-bd58-4f1d-a8c0-7f43eaee6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(params):\n",
    "    return processes.gaussian(\n",
    "        vmap(means.zero()),\n",
    "        vmap(vmap(kernels.scale(bijector.forward(params['amplitude']), kernels.rbf(bijector.forward(params['length_scale']))), in_axes=(None, 0)), in_axes=(0, None)),\n",
    "        bijector.forward(params['noise']),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecb90-c62a-4d25-97eb-fb601691843f",
   "metadata": {},
   "source": [
    "Next we initialise the models' hyperparameters, the optimizer, and fit the model to the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d076f8a7-24cf-4155-80d9-21b796531544",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  'amplitude': jnp.zeros(()),\n",
    "  'length_scale': jnp.zeros(()),\n",
    "  'noise': jnp.array(-5.),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "337ea1d7-544e-4071-a08e-9e59d68ad203",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(0.01)\n",
    "opt_state = optimizer.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89522430-b013-483c-a28d-8b3a5a111e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, iteration):\n",
    "  def loss_fn(params):            \n",
    "    loc, scale = process(params).prior(x_train)\n",
    "    return -scipy.stats.multivariate_normal.logpdf(y_train, loc, scale)\n",
    "\n",
    "  loss, grads = value_and_grad(loss_fn)(state[0])\n",
    "  updates, opt_state = optimizer.update(grads, state[1])\n",
    "  params = optax.apply_updates(state[0], updates)\n",
    "\n",
    "  return (params, opt_state), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b8b89cb-1769-4d35-9583-7515325b0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "(next_params, next_opt_state), history = lax.scan(\n",
    "    jit(train_step),\n",
    "    (params, opt_state),\n",
    "    jnp.arange(500)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283a8d8-0076-42e3-abf9-c2a253e5a1a3",
   "metadata": {},
   "source": [
    "## Constructing and optimizing an acquisition function\n",
    "\n",
    "As a final step we use the posterior of our Gaussian Process model to construct the Upper Bound Confidence acquisition function which we maximize using the BFGS optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56896400-ef48-429a-b149-ff517cf0ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "acqusition = acquisitions.upper_confidence_bound(\n",
    "    2,\n",
    "    partial(process(next_params).posterior, observation_index_points=x_train, observations=y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71e6ed01-1589-42c0-a04f-957913cd55b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates, scores = maximizers.bfgs(50)(acqusition, space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
